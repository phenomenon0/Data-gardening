{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FBref Player Stats Web Scraping\n",
    "##### Notebook to scrape raw data from [FBref](https://fbref.com/en/) via [StatsBomb](https://statsbomb.com/) using the [pandas](http://pandas.pydata.org/)\n",
    "\n",
    "### By [Edd Webster](https://www.twitter.com/eddwebster)\n",
    "Notebook first written: 31/08/2020<br>\n",
    "Notebook last updated: 31/08/2021\n",
    "\n",
    "![title](../../img/fbref-logo-banner.png)\n",
    "\n",
    "![title](../../img/stats-bomb-logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a id='sectionintro'></a>\n",
    "\n",
    "## <a id='import_libraries'>Introduction</a>\n",
    "This notebook scrapes player statstics data from [FBref](https://fbref.com/en/) via [StatsBomb](https://statsbomb.com/), using the [pandas](http://pandas.pydata.org/) [`read_html`](https://pandas.pydata.org/docs/reference/api/pandas.read_html.html) function [pandas](http://pandas.pydata.org/) for webscraping and data manipulation through DataFrames.\n",
    "\n",
    "For more information about this notebook and the author, I am available through all the following channels:\n",
    "*    [eddwebster.com](https://www.eddwebster.com/);\n",
    "*    edd.j.webster@gmail.com;\n",
    "*    [@eddwebster](https://www.twitter.com/eddwebster);\n",
    "*    [linkedin.com/in/eddwebster](https://www.linkedin.com/in/eddwebster/);\n",
    "*    [github/eddwebster](https://github.com/eddwebster/);\n",
    "*    [public.tableau.com/profile/edd.webster](https://public.tableau.com/profile/edd.webster);\n",
    "*    [kaggle.com/eddwebster](https://www.kaggle.com/eddwebster); and\n",
    "*    [hackerrank.com/eddwebster](https://www.hackerrank.com/eddwebster).\n",
    "\n",
    "![title](../../img/fifa21eddwebsterbanner.png)\n",
    "\n",
    "The accompanying GitHub repository for this notebook can be found [here](https://github.com/eddwebster/football_analytics) and a static version of this notebook can be found [here](https://nbviewer.jupyter.org/github/eddwebster/football_analytics/blob/master/notebooks/A%29%20Web%20Scraping/FBref%20Web%20Scraping%20and%20Parsing.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a id='sectioncontents'></a>\n",
    "\n",
    "## <a id='notebook_contents'>Notebook Contents</a>\n",
    "1.    [Notebook Dependencies](#section1)<br>\n",
    "2.    [Project Brief](#section2)<br>\n",
    "3.    [Data Scraping](#section3)<br>\n",
    "      1.    [Introduction](#section3.1)<br>\n",
    "      2.    [Outfielder Players](#section3.2)<br>\n",
    "            1.    [Data Dictionary](#section3.2.1)<br>\n",
    "            2.    [Creating the DataFrame](#section3.2.2)<br>\n",
    "            3.    [Initial Data Handling](#section3.2.3)<br>\n",
    "            4.    [Export the Raw DataFrame](#section3.2.4)<br>\n",
    "      3.    [Goalkeepers](#section3.3)<br>\n",
    "            1.    [Data Dictionary](#section3.3.1)<br>\n",
    "            2.    [Creating the DataFrame](#section3.3.2)<br>\n",
    "            3.    [Initial Data Handling](#section3.3.3)<br>\n",
    "            4.    [Export the Raw DataFrame](#section3.3.4)<br> \n",
    "4.    [Summary](#section4)<br>\n",
    "5.    [Next Steps](#section5)<br>\n",
    "6.    [References](#section6)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a id='section1'></a>\n",
    "\n",
    "## <a id='#section1'>1. Notebook Dependencies</a>\n",
    "\n",
    "This notebook was written using [Python 3](https://docs.python.org/3.7/) and requires the following libraries:\n",
    "*    [`Jupyter notebooks`](https://jupyter.org/) for this notebook environment with which this project is presented;\n",
    "*    [`NumPy`](http://www.numpy.org/) for multidimensional array computing; and\n",
    "*    [`pandas`](http://pandas.pydata.org/) for data analysis and manipulation.\n",
    "\n",
    "All packages used for this notebook except for BeautifulSoup can be obtained by downloading and installing the [Conda](https://anaconda.org/anaconda/conda) distribution, available on all platforms (Windows, Linux and Mac OSX). Step-by-step guides on how to install Anaconda can be found for Windows [here](https://medium.com/@GalarnykMichael/install-python-on-windows-anaconda-c63c7c3d1444) and Mac [here](https://medium.com/@GalarnykMichael/install-python-on-mac-anaconda-ccd9f2014072), as well as in the Anaconda documentation itself [here](https://docs.anaconda.com/anaconda/install/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "# Python â‰¥3.5 (ideally)\n",
    "import platform\n",
    "import sys, getopt\n",
    "assert sys.version_info >= (3, 5)\n",
    "import csv\n",
    "\n",
    "# Import Dependencies\n",
    "%matplotlib inline\n",
    "\n",
    "# Math Operations\n",
    "import numpy as np\n",
    "from math import pi\n",
    "\n",
    "# Datetime\n",
    "import datetime\n",
    "from datetime import date\n",
    "import time\n",
    "\n",
    "# Data Preprocessing\n",
    "import pandas as pd\n",
    "import pandas_profiling as pp\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import glob\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "# Reading directories\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Working with JSON\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Web Scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Data Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import missingno as msno\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Display in Jupyter\n",
    "from IPython.display import Image, YouTubeVideo\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "print('Setup Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.7.6\n",
      "NumPy: 1.20.3\n",
      "pandas: 1.3.2\n",
      "matplotlib: 3.4.2\n"
     ]
    }
   ],
   "source": [
    "# Python / module versions used here for reference\n",
    "print('Python: {}'.format(platform.python_version()))\n",
    "print('NumPy: {}'.format(np.__version__))\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('matplotlib: {}'.format(mpl.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up initial paths to subfolders\n",
    "base_dir = os.path.join('..', '..')\n",
    "data_dir = os.path.join(base_dir, 'data')\n",
    "data_dir_fbref = os.path.join(base_dir, 'data', 'fbref')\n",
    "img_dir = os.path.join(base_dir, 'img')\n",
    "fig_dir = os.path.join(base_dir, 'img', 'fig')\n",
    "video_dir = os.path.join(base_dir, 'video')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined variables\n",
    "\n",
    "## Define today's date\n",
    "today = datetime.datetime.now().strftime('%d/%m/%Y').replace('/', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define league names and their IDs\n",
    "dict_league_names = {'Premier-League': '9',\n",
    "                     'Ligue-1': '13',\n",
    "                     'Bundesliga': '20',\n",
    "                     'Serie-A': '11',\n",
    "                     'La-Liga': '12',\n",
    "                     'Major-League-Soccer': '22',\n",
    "                     'Big-5-European-Leagues': 'Big5'\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined Lists\n",
    "\n",
    "## Define list of long names for 'Big 5' European Leagues and MLS\n",
    "lst_league_names_long = ['Premier-League', 'Ligue-1', 'Bundesliga', 'Serie-A', 'La-Liga', 'Major-League-Soccer', 'Big-5-European-Leagues']\n",
    "\n",
    "## Define seasons to scrape\n",
    "lst_seasons = ['2017-2018', '2018-2019', '2019-2020', '2020-2021', '2021-2022']\n",
    "\n",
    "## Define list of folders\n",
    "lst_folders = ['raw', 'engineered', 'reference']\n",
    "\n",
    "## Define list of data types\n",
    "lst_data_types = ['goalkeeper', 'outfield', 'team']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Functions (Scrapers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for scraping a defined season and competition of FBref player data\n",
    "def get_fbref_player_stats(lst_league_names, lst_seasons):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to scrape player stats from FBref.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ## Define list of league names\n",
    "    league_names_long = lst_league_names\n",
    "    \n",
    "    \n",
    "    ## Define seasons to scrape\n",
    "    seasons = lst_seasons\n",
    "    \n",
    "    \n",
    "    ## Start timer\n",
    "    tic = datetime.datetime.now()\n",
    "    \n",
    "    \n",
    "    ## Print time scraping started\n",
    "    print(f'Scraping started at: {tic}')\n",
    "    \n",
    "    \n",
    "    ## Scrape information for each player\n",
    "    for season in seasons:\n",
    "\n",
    "        ### Print message\n",
    "        print(f'Scraping started for the {season} season...')\n",
    "\n",
    "        ### Loop through leagues\n",
    "        for league_name_long in league_names_long:\n",
    "            \n",
    "            #### Determine league short name from the league names dictionary\n",
    "            league_name_short = [v for k,v in dict_league_names.items() if k == league_name_long][0]\n",
    "            \n",
    "            #### Save Player URL List (if not already saved)\n",
    "            if not os.path.exists(os.path.join(data_dir_fbref + f'/raw/outfield/{league_name_long}/{season}/fbref_outfield_player_stats_{league_name_long}_{season}_latest.csv')):\n",
    "\n",
    "                ##### Scraping\n",
    "\n",
    "                ##### Print statement\n",
    "                print(f'Scraping started for player stats data for {league_name_long} league for the {season} season...')\n",
    "\n",
    "                ##### Standard stats\n",
    "                print(f'Scraping Standard stats...')\n",
    "                url_std_stats = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fstats%2Fplayers%2F{season}-{league_name_long}&div=div_stats_standard'\n",
    "                df_std_stats = pd.read_html(url_std_stats, header=1)[0]\n",
    "\n",
    "                ##### Goalkeeper stats\n",
    "                #print(f'Scraping Goalkeeper stats...')\n",
    "                #url_keepers = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fkeepers%2Fplayers%2F{season}-{league_name_long}&div=div_stats_keeper'\n",
    "                #df_keepers = pd.read_html(url_keepers, header=1)[0]\n",
    "\n",
    "                ##### Advanced Goalkeeper stats\n",
    "                #print(f'Scraping Advanced Goalkeeper stats...')\n",
    "                #url_keepers_adv = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fkeepersadv%2Fplayers%2F{season}-{league_name_long}&div=div_stats_keeper_adv'\n",
    "                #df_keepers_adv = pd.read_html(url_keepers_adv, header=1)[0]\n",
    "\n",
    "                ##### Shooting stats\n",
    "                print(f'Scraping Shooting stats...')\n",
    "                url_shooting = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fshooting%2Fplayers%2F{season}-{league_name_long}&div=div_stats_shooting'\n",
    "                df_shooting = pd.read_html(url_shooting, header=1)[0]\n",
    "\n",
    "                ##### Passing stats\n",
    "                print(f'Scraping Passing stats...')\n",
    "                url_passing = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fpassing%2Fplayers%2F{season}-{league_name_long}&div=div_stats_passing'\n",
    "                df_passing = pd.read_html(url_passing, header=1)[0]\n",
    "\n",
    "                ##### Pass Types stats\n",
    "                print(f'Scraping Pass Types stats...')\n",
    "                url_passing_types = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fpassing_types%2Fplayers%2F{season}-{league_name_long}&div=div_stats_passing_types'\n",
    "                df_passing_types = pd.read_html(url_passing_types, header=1)[0]\n",
    "\n",
    "                ##### Goals and Shot Creation stats\n",
    "                print(f'Scraping Goals and Shot Creation stats...')\n",
    "                url_gca = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fgca%2Fplayers%2F{season}-{league_name_long}&div=div_stats_gca'\n",
    "                df_gca = pd.read_html(url_gca, header=1)[0]\n",
    "\n",
    "                ##### Defensive Actions stats\n",
    "                print(f'Scraping Defensive Actions stats...')\n",
    "                url_defense = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fdefense%2Fplayers%2F{season}-{league_name_long}&div=div_stats_defense'\n",
    "                df_defense = pd.read_html(url_defense, header=1)[0]\n",
    "\n",
    "                ##### Possession stats\n",
    "                print(f'Scraping Possession stats...')\n",
    "                url_possession = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fpossession%2Fplayers%2F{season}-{league_name_long}&div=div_stats_possession'\n",
    "                df_possession = pd.read_html(url_possession, header=1)[0]\n",
    "\n",
    "                ##### Playing Time stats\n",
    "                print(f'Scraping Playing Time stats...')\n",
    "                url_playing_time = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fplayingtime%2Fplayers%2F{season}-{league_name_long}&div=div_stats_playing_time'\n",
    "                df_playing_time = pd.read_html(url_playing_time, header=1)[0]\n",
    "\n",
    "                ##### Miscellaneous stats\n",
    "                print(f'Scraping Miscellaneous stats...')\n",
    "                url_misc = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fmisc%2Fplayers%2F{season}-{league_name_long}&div=div_stats_misc'\n",
    "                df_misc = pd.read_html(url_misc, header=1)[0]\n",
    "\n",
    "                ##### Concatenate defined individual DataFrames\n",
    "                \n",
    "                ####### Define DataFrames to be concatenated side-by-side (not all of them)\n",
    "                lst_dfs = [df_std_stats, df_shooting, df_passing, df_passing_types, df_gca, df_defense, df_possession]\n",
    "\n",
    "                ###### Concatenate DataFrames side-by-side (indicated in list above)\n",
    "                df_all = pd.concat(lst_dfs, axis=1)\n",
    "\n",
    "                ###### Drop duplicate columns\n",
    "                df_all = df_all.loc[:,~df_all.columns.duplicated()]\n",
    "\n",
    "                ###### Drop duplicate rows\n",
    "                df_all = df_all.drop_duplicates()\n",
    "                \n",
    "                ##### Left join defined individual DataFrames\n",
    "                \n",
    "                ####### Define join conditions\n",
    "                conditions_join = ['Player', 'Nation', 'Pos', 'Squad', 'Comp']\n",
    "\n",
    "                ###### Left join Playing Time data\n",
    "                df_all = pd.merge(df_all, df_playing_time, left_on=conditions_join, right_on=conditions_join, how='left')\n",
    "\n",
    "                ###### Remove duplicate columns after join (contain '_y') and remove '_x' suffix from kept columns\n",
    "                df_all = df_all[df_all.columns.drop(list(df_all.filter(regex='_y')))]\n",
    "                df_all.columns = df_all.columns.str.replace('_x','')\n",
    "                \n",
    "                ###### Drop duplicate rows\n",
    "                df_all = df_all.drop_duplicates()\n",
    "\n",
    "                ###### Left join Misc data\n",
    "                df_all = pd.merge(df_all, df_misc, left_on=conditions_join, right_on=conditions_join, how='left')\n",
    "\n",
    "                ###### Remove duplicate columns after join (contain '_y') and remove '_x' suffix from kept columns\n",
    "                df_all = df_all[df_all.columns.drop(list(df_all.filter(regex='_y')))]\n",
    "                df_all.columns = df_all.columns.str.replace('_x','')\n",
    "                \n",
    "                ###### Drop duplicate rows\n",
    "                df_all = df_all.drop_duplicates()\n",
    "                \n",
    "                \n",
    "                ##### Engineer DataFrames\n",
    "                \n",
    "                ###### Take first two digits of age - fixes current season issue with extra values\n",
    "                df_all['Age'] = df_all['Age'].astype(str).str[:2]\n",
    "                \n",
    "                ###### Create columns for league code and season\n",
    "                df_all['League Name'] = league_name_long\n",
    "                df_all['League ID'] = league_name_short\n",
    "                df_all['Season'] = season              \n",
    "\n",
    "                ###### Drop duplicates\n",
    "                df_all = df_all.drop_duplicates()\n",
    "\n",
    "                \n",
    "                ##### Save DataFrame\n",
    "                df_all.to_csv(data_dir_fbref + f'/raw/outfield/{league_name_long}/{season}/fbref_outfield_player_stats_{league_name_long}_{season}_latest.csv', index=None, header=True, encoding='utf-8')        \n",
    "                \n",
    "                ##### Export a copy to the 'archive' subfolder, including the date\n",
    "                df_all.to_csv(data_dir_fbref + f'/raw/outfield/{league_name_long}/{season}/archive/fbref_outfield_player_stats_{league_name_long}_{season}_last_updated_{today}.csv', index=None, header=True, encoding='utf-8')        \n",
    "                \n",
    "                \n",
    "                ##### Print statement for league and season\n",
    "                print(f'All player stats data for the {league_name_long} league for {season} season scraped and saved.')\n",
    "             \n",
    "            \n",
    "            #### Load player stats data (if already saved)\n",
    "            else:\n",
    "\n",
    "                ##### Print statement\n",
    "                print(f'Player stats data for the {league_name_long} league for the {season} season already saved as a CSV file.')         \n",
    "\n",
    "                \n",
    "    ## End timer\n",
    "    toc = datetime.datetime.now()\n",
    "    \n",
    "    \n",
    "    ## Print time scraping ended\n",
    "    print(f'Scraping ended at: {toc}')\n",
    "\n",
    "    \n",
    "    ## Calculate time take\n",
    "    total_time = (toc-tic).total_seconds()\n",
    "    print(f'Time taken to scrape the player stats data for {len(league_names_long)} leagues for {len(seasons)} seasons is: {total_time/60:0.2f} minutes.')\n",
    "\n",
    "    \n",
    "    ## Unify individual CSV files as a single DataFrame\n",
    "    \n",
    "    ### Show files in directory\n",
    "    all_files = glob.glob(os.path.join(data_dir_fbref + f'/raw/outfield/*/*/fbref_outfield_player_stats_*_*_latest.csv'))\n",
    "    \n",
    "    ### Create an empty list of Players URLs\n",
    "    lst_player_stats_all = []\n",
    "\n",
    "    ### Loop through list of files and read into temporary DataFrames\n",
    "    for filename in all_files:\n",
    "        df_temp = pd.read_csv(filename, index_col=None, header=0)\n",
    "        lst_player_stats_all.append(df_temp)\n",
    "\n",
    "    ### Concatenate the files into a single DataFrame\n",
    "    df_fbref_player_stats_all = pd.concat(lst_player_stats_all, axis=0, ignore_index=True)\n",
    "    \n",
    "    ### Drop header row of each concatenated  DataFrame (contains 'Rk', 'Rk' column)\n",
    "    df_fbref_player_stats_all = df_fbref_player_stats_all[~df_fbref_player_stats_all['Rk'].str.contains('Rk')]\n",
    "    \n",
    "    ### Drop 'Rk' column\n",
    "    df_fbref_player_stats_all = df_fbref_player_stats_all.drop(['Rk'], axis=1)\n",
    "    \n",
    "    ### Reset index\n",
    "    #df_fbref_player_stats_all = df_fbref_player_stats_all.reset_index()\n",
    "    \n",
    "    ### Sort DataFrame\n",
    "    df_fbref_player_stats_all = df_fbref_player_stats_all.sort_values(['League Name', 'Season', 'Player'], ascending=[True, True, True])\n",
    "\n",
    "    \n",
    "    ## Export DataFrame\n",
    "    \n",
    "    ###\n",
    "    df_fbref_player_stats_all.to_csv(data_dir_fbref + f'/raw/outfield/fbref_outfield_player_stats_combined_latest.csv', index=None, header=True, encoding='utf-8')\n",
    "    \n",
    "    ### Save a copy to archive folder (dated)\n",
    "    df_fbref_player_stats_all.to_csv(data_dir_fbref + f'/raw/outfield/archive/fbref_outfield_player_stats_combined_last_updated_{today}.csv', index=None, header=True, encoding='utf-8')\n",
    "    \n",
    "    \n",
    "    ## Distinct number of players\n",
    "    total_players = df_fbref_player_stats_all['Player'].nunique()\n",
    "\n",
    "\n",
    "    ## Print statement\n",
    "    print(f'Player stats DataFrame contains {total_players} players.')\n",
    "    \n",
    "    \n",
    "    ## Return final list of Player URLs\n",
    "    return(df_fbref_player_stats_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for scraping a defined season and competition of FBref player data\n",
    "def get_fbref_goalkeeper_stats(lst_league_names, lst_seasons):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to scrape goalkeeper stats from FBref.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ## Define list of league names\n",
    "    league_names_long = lst_league_names\n",
    "    \n",
    "    \n",
    "    ## Define seasons to scrape\n",
    "    seasons = lst_seasons\n",
    "    \n",
    "    \n",
    "    ## Start timer\n",
    "    tic = datetime.datetime.now()\n",
    "    \n",
    "    \n",
    "    ## Print time scraping started\n",
    "    print(f'Scraping started at: {tic}')\n",
    "    \n",
    "    \n",
    "    ## Scrape information for each player\n",
    "    for season in seasons:\n",
    "\n",
    "        ### Print message\n",
    "        print(f'Scraping started for the {season} season...')\n",
    "\n",
    "        ### Loop through leagues\n",
    "        for league_name_long in league_names_long:\n",
    "            \n",
    "            #### Determine league short name from the league names dictionary\n",
    "            league_name_short = [v for k,v in dict_league_names.items() if k == league_name_long][0]\n",
    "            \n",
    "            #### Save Player URL List (if not already saved)\n",
    "            if not os.path.exists(os.path.join(data_dir_fbref + f'/raw/goalkeeper/{league_name_long}/{season}/fbref_goalkeeper_stats_{league_name_long}_{season}_latest.csv', encoding='utf-8')):\n",
    "\n",
    "                ##### Scraping\n",
    "\n",
    "                ##### Print statement\n",
    "                print(f'Scraping started for goalkeeper stats data for {league_name_long} league for the {season} season...')\n",
    "\n",
    "                ##### Standard stats\n",
    "                print(f'Scraping Standard stats...')\n",
    "                url_std_stats = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fstats%2Fplayers%2F{season}-{league_name_long}&div=div_stats_standard'\n",
    "                df_std_stats = pd.read_html(url_std_stats, header=1)[0]\n",
    "\n",
    "                ##### Goalkeeper stats\n",
    "                print(f'Scraping Goalkeeper stats...')\n",
    "                url_keepers = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fkeepers%2Fplayers%2F{season}-{league_name_long}&div=div_stats_keeper'\n",
    "                df_keepers = pd.read_html(url_keepers, header=1)[0]\n",
    "\n",
    "                ##### Advanced Goalkeeper stats\n",
    "                print(f'Scraping Advanced Goalkeeper stats...')\n",
    "                url_keepers_adv = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fkeepersadv%2Fplayers%2F{season}-{league_name_long}&div=div_stats_keeper_adv'\n",
    "                df_keepers_adv = pd.read_html(url_keepers_adv, header=1)[0]\n",
    "\n",
    "                ##### Playing Time stats\n",
    "                print(f'Scraping Playing Time stats...')\n",
    "                url_playing_time = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fplayingtime%2Fplayers%2F{season}-{league_name_long}&div=div_stats_playing_time'\n",
    "                df_playing_time = pd.read_html(url_playing_time, header=1)[0]\n",
    "\n",
    "                ##### Miscellaneous stats\n",
    "                print(f'Scraping Miscellaneous stats...')\n",
    "                url_misc = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fmisc%2Fplayers%2F{season}-{league_name_long}&div=div_stats_misc'\n",
    "                df_misc = pd.read_html(url_misc, header=1)[0]\n",
    "\n",
    "                ##### Concatenate defined individual DataFrames\n",
    "                \n",
    "                ####### Define DataFrames to be concatenated side-by-side (not all of them)\n",
    "                lst_dfs = [df_keepers, df_keepers_adv]\n",
    "\n",
    "                ###### Concatenate DataFrames side-by-side (indicated in list above)\n",
    "                df_all = pd.concat(lst_dfs, axis=1)\n",
    "\n",
    "                ###### Drop duplicate columns\n",
    "                df_all = df_all.loc[:,~df_all.columns.duplicated()]\n",
    "\n",
    "                ###### Drop duplicate rows\n",
    "                df_all = df_all.drop_duplicates()\n",
    "                \n",
    "                ##### Left join defined individual DataFrames\n",
    "                \n",
    "                ####### Define join conditions\n",
    "                conditions_join = ['Player', 'Nation', 'Pos', 'Squad', 'Comp']\n",
    "\n",
    "                ###### Left join Standard Stats data\n",
    "                df_all = pd.merge(df_all, df_std_stats, left_on=conditions_join, right_on=conditions_join, how='left')\n",
    "\n",
    "                ###### Remove duplicate columns after join (contain '_y') and remove '_x' suffix from kept columns\n",
    "                df_all = df_all[df_all.columns.drop(list(df_all.filter(regex='_y')))]\n",
    "                df_all.columns = df_all.columns.str.replace('_x','')\n",
    "                \n",
    "                ###### Drop duplicate rows\n",
    "                df_all = df_all.drop_duplicates()\n",
    "                \n",
    "                ###### Left join Playing Time data\n",
    "                df_all = pd.merge(df_all, df_playing_time, left_on=conditions_join, right_on=conditions_join, how='left')\n",
    "\n",
    "                ###### Remove duplicate columns after join (contain '_y') and remove '_x' suffix from kept columns\n",
    "                df_all = df_all[df_all.columns.drop(list(df_all.filter(regex='_y')))]\n",
    "                df_all.columns = df_all.columns.str.replace('_x','')\n",
    "                \n",
    "                ###### Drop duplicate rows\n",
    "                df_all = df_all.drop_duplicates()\n",
    "\n",
    "                ###### Left join Misc data\n",
    "                df_all = pd.merge(df_all, df_misc, left_on=conditions_join, right_on=conditions_join, how='left')\n",
    "\n",
    "                ###### Remove duplicate columns after join (contain '_y') and remove '_x' suffix from kept columns\n",
    "                df_all = df_all[df_all.columns.drop(list(df_all.filter(regex='_y')))]\n",
    "                df_all.columns = df_all.columns.str.replace('_x','')\n",
    "                \n",
    "                ###### Drop duplicate rows\n",
    "                df_all = df_all.drop_duplicates()\n",
    "                \n",
    "                \n",
    "                ##### Engineer DataFrames\n",
    "                \n",
    "                ###### Take first two digits of age - fixes current season issue with extra values\n",
    "                df_all['Age'] = df_all['Age'].astype(str).str[:2]\n",
    "                \n",
    "                ###### Create columns for league code and season\n",
    "                df_all['League Name'] = league_name_long\n",
    "                df_all['League ID'] = league_name_short\n",
    "                df_all['Season'] = season              \n",
    "\n",
    "                ###### Drop duplicates\n",
    "                df_all = df_all.drop_duplicates()\n",
    "\n",
    "                \n",
    "                ##### Save DataFrame\n",
    "                df_all.to_csv(data_dir_fbref + f'/raw/goalkeeper/{league_name_long}/{season}/fbref_goalkeeper_stats_{league_name_long}_{season}_latest.csv', index=None, header=True, encoding='utf-8')        \n",
    "                \n",
    "                ##### Export a copy to the 'archive' subfolder, including the date\n",
    "                df_all.to_csv(data_dir_fbref + f'/raw/goalkeeper/{league_name_long}/{season}/archive/fbref_goalkeeper_stats_{league_name_long}_{season}_last_updated_{today}.csv', index=None, header=True, encoding='utf-8')        \n",
    "                \n",
    "                \n",
    "                ##### Print statement for league and season\n",
    "                print(f'All Goalkeeper stats data for the {league_name_long} league for {season} season scraped and saved.')\n",
    "             \n",
    "            \n",
    "            #### Load goalkeeper stats data (if already saved)\n",
    "            else:\n",
    "\n",
    "                ##### Print statement\n",
    "                print(f'Goalkeeper stats data for the {league_name_long} league for the {season} season already saved as a CSV file.')         \n",
    "\n",
    "                \n",
    "    ## End timer\n",
    "    toc = datetime.datetime.now()\n",
    "    \n",
    "    \n",
    "    ## Print time scraping ended\n",
    "    print(f'Scraping ended at: {toc}')\n",
    "\n",
    "    \n",
    "    ## Calculate time take\n",
    "    total_time = (toc-tic).total_seconds()\n",
    "    print(f'Time taken to scrape the goalkeeper stats data for {len(league_names_long)} leagues for {len(seasons)} seasons is: {total_time/60:0.2f} minutes.')\n",
    "\n",
    "    \n",
    "    ## Unify individual CSV files as a single DataFrame\n",
    "    \n",
    "    ### Show files in directory\n",
    "    all_files = glob.glob(os.path.join(data_dir_fbref + f'/raw/goalkeeper/*/*/fbref_goalkeeper_stats_*_*_latest.csv'))\n",
    "    \n",
    "    ### Create an empty list of Players URLs\n",
    "    lst_goalkeeper_stats_all = []\n",
    "\n",
    "    ### Loop through list of files and read into temporary DataFrames\n",
    "    for filename in all_files:\n",
    "        df_temp = pd.read_csv(filename, index_col=None, header=0)\n",
    "        lst_goalkeeper_stats_all.append(df_temp)\n",
    "\n",
    "    ### Concatenate the files into a single DataFrame\n",
    "    df_fbref_goalkeeper_stats_all = pd.concat(lst_goalkeeper_stats_all, axis=0, ignore_index=True)\n",
    "    \n",
    "    ### Drop header row of each concatenated  DataFrame (contains 'Rk', 'Rk' column)\n",
    "    df_fbref_goalkeeper_stats_all = df_fbref_goalkeeper_stats_all[~df_fbref_goalkeeper_stats_all['Rk'].str.contains('Rk')]\n",
    "    \n",
    "    ### Drop 'Rk' column\n",
    "    df_fbref_goalkeeper_stats_all = df_fbref_goalkeeper_stats_all.drop(['Rk'], axis=1)\n",
    "    \n",
    "    ### Reset index\n",
    "    #df_fbref_goalkeeper_stats_all = df_fbref_goalkeeper_stats_all.reset_index()\n",
    "    \n",
    "    ### Sort DataFrame\n",
    "    df_fbref_goalkeeper_stats_all = df_fbref_goalkeeper_stats_all.sort_values(['League Name', 'Season', 'Player'], ascending=[True, True, True])\n",
    "\n",
    "    \n",
    "    ## Export DataFrame\n",
    "    \n",
    "    ###\n",
    "    df_fbref_goalkeeper_stats_all.to_csv(data_dir_fbref + f'/raw/goalkeeper/fbref_goalkeeper_stats_combined_latest.csv', index=None, header=True, encoding='utf-8')\n",
    "    \n",
    "    ### Save a copy to archive folder (dated)\n",
    "    df_fbref_goalkeeper_stats_all.to_csv(data_dir_fbref + f'/raw/goalkeeper/archive/fbref_goalkeeper_stats_combined_last_updated_{today}.csv', index=None, header=True, encoding='utf-8')\n",
    "    \n",
    "    \n",
    "    ## Distinct number of goalkeepers\n",
    "    total_players = df_fbref_goalkeeper_stats_all['Player'].nunique()\n",
    "\n",
    "\n",
    "    ## Print statement\n",
    "    print(f'Goalkeeper stats DataFrame contains {total_players} players.')\n",
    "    \n",
    "    \n",
    "    ## Return final list of Player URLs\n",
    "    return(df_fbref_goalkeeper_stats_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Directory Structure\n",
    "Create folders and subfolders for data, if not already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the data directory structure\n",
    "for folder in lst_folders:\n",
    "    path = os.path.join(data_dir_fbref, folder)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "        for data_types in lst_data_types:\n",
    "            path = os.path.join(data_dir_fbref, folder, data_types)\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "                os.mkdir(os.path.join(path, 'archive'))\n",
    "                for league in lst_league_names_long:\n",
    "                    path = os.path.join(data_dir_fbref, folder, data_types, league)\n",
    "                    if not os.path.exists(path):\n",
    "                        os.mkdir(path)\n",
    "                        for season in lst_seasons:\n",
    "                            path = os.path.join(data_dir_fbref, folder, data_types, league, season)\n",
    "                            if not os.path.exists(path):\n",
    "                                os.mkdir(path)\n",
    "                                os.mkdir(os.path.join(path, 'archive'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all columns of pandas DataFrames\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section2'></a>\n",
    "\n",
    "## <a id='#section2'>2. Project Brief</a>\n",
    "This Jupyter notebook is part of a series of notebooks, to scrape, parse, engineer, and unify datasets, that can be used for modeling purposes.\n",
    "\n",
    "This particular notebook is one of several web scraping notebooks, that takes data from [FBref](https://fbref.com/en/), provided by [StatsBomb](https://statsbomb.com/), and scrapes it using the [pandas](http://pandas.pydata.org/) [`read_html`](https://pandas.pydata.org/docs/reference/api/pandas.read_html.html) function and manipulates it as Dataframe.\n",
    "\n",
    "This notebook, along with the other notebooks in this project workflow are shown in the following diagram:\n",
    "\n",
    "![roadmap](../../img/football_analytics_data_roadmap.png)\n",
    "\n",
    "Links to these notebooks in the [`football_analytics`](https://github.com/eddwebster/football_analytics) GitHub repository can be found at the following:\n",
    "*    [1. Webscraping](https://github.com/eddwebster/football_analytics/tree/master/notebooks/1_data_scraping)\n",
    "     +    [FBref Player Stats Webscraping](https://github.com/eddwebster/football_analytics/blob/master/notebooks/1_data_scraping/FBref%20Player%20Stats%20Web%20Scraping.ipynb)\n",
    "     +    [TransferMarket Player Bio and Status Webscraping](https://github.com/eddwebster/football_analytics/blob/master/notebooks/1_data_scraping/TransferMarkt%20Player%20Bio%20and%20Status%20Web%20Scraping.ipynb)\n",
    "     +    [TransferMarket Player Valuation Webscraping](https://github.com/eddwebster/football_analytics/blob/master/notebooks/1_data_scraping/TransferMarkt%20Player%20Valuation%20Web%20Scraping.ipynb)\n",
    "     +    [TransferMarkt Player Recorded Transfer Fees Webscraping](https://github.com/eddwebster/football_analytics/blob/master/notebooks/1_data_scraping/TransferMarkt%20Player%20Recorded%20Transfer%20Fees%20Webscraping.ipynb)\n",
    "     +    [Capology Player Salary Webscraping](https://github.com/eddwebster/football_analytics/blob/master/notebooks/1_data_scraping/Capology%20Player%20Salary%20Web%20Scraping.ipynb)\n",
    "     +    [FBref Team Stats Webscraping](https://github.com/eddwebster/football_analytics/blob/master/notebooks/1_data_scraping/FBref%20Team%20Stats%20Web%20Scraping.ipynb)\n",
    "*    [2. Data Parsing](https://github.com/eddwebster/football_analytics/tree/master/notebooks/2_data_parsing)\n",
    "     +    [ELO Team Ratings Data Parsing](https://github.com/eddwebster/football_analytics/blob/master/notebooks/2_data_parsing/ELO%20Team%20Ratings%20Data%20Parsing.ipynb)\n",
    "*    [3. Data Engineering](https://github.com/eddwebster/football_analytics/tree/master/notebooks/3_data_engineering)\n",
    "     +    [FBref Player Stats Data Engineering](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/FBref%20Player%20Stats%20Data%20Engineering.ipynb)\n",
    "     +    [TransferMarket Player Bio and Status Data Engineering](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/TransferMarkt%20Player%20Bio%20and%20Status%20Data%20Engineering.ipynb)\n",
    "     +    [TransferMarket Player Valuation Data Engineering](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/TransferMarkt%20Player%20Valuation%20Data%20Engineering.ipynb)\n",
    "     +    [TransferMarkt Player Recorded Transfer Fees Data Engineering](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/TransferMarkt%20Player%20Recorded%20Transfer%20Fees%20Data%20Engineering.ipynb)\n",
    "     +    [Capology Player Salary Data Engineering](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/Capology%20Player%20Salary%20Data%20Engineering.ipynb)\n",
    "     +    [FBref Team Stats Data Engineering](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/FBref%20Team%20Stats%20Data%20Engineering.ipynb)\n",
    "     +    [ELO Team Ratings Data Parsing](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/ELO%20Team%20Ratings%20Data%20Parsing.ipynb)\n",
    "     +    [TransferMarkt Team Recorded Transfer Fee Data Engineering](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/TransferMarkt%20Team%20Recorded%20Transfer%20Fee%20Data%20Engineering.ipynb) (aggregated from [TransferMarkt Player Recorded Transfer Fees notebook](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/TransferMarkt%20Player%20Recorded%20Transfer%20Fees%20Data%20Engineering.ipynb))\n",
    "     +    [Capology Team Salary Data Engineering](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/Capology%20Team%20Salary%20Data%20Engineering.ipynb) (aggregated from [Capology Player Salary notebook](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/Capology%20Player%20Salary%20Data%20Engineering.ipynb))\n",
    "*    [4. Data Unification](https://github.com/eddwebster/football_analytics/tree/master/notebooks/4_data_unification)\n",
    "*    [5. Modeling and Data Analysis]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section3'></a>\n",
    "\n",
    "## <a id='#section3'>3. Data Scraping</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3.1'></a>\n",
    "\n",
    "### <a id='#section3.1'>3.1. Introduction</a>\n",
    "This Data Sources section has been has been split into two subsections - outfielder, and goalkeeper.\n",
    "\n",
    "The data needs to be scraped, converted to a pandas DataFrame ([Section 3](#section3)) and cleaned in the Data Engineering section ([Section 4](#section4)).\n",
    "\n",
    "We'll be using the [pandas](http://pandas.pydata.org/) library to import our data to this workbook as a DataFrame.\n",
    "\n",
    "From FBref it is also possible to scrape team data, this is covered separately in the following notebook in the [Data Scraping](https://github.com/eddwebster/football_analytics/tree/master/notebooks/1_data_scraping) folder [[link](https://github.com/eddwebster/football_analytics/blob/master/notebooks/1_data_scraping/FBref%20Team%20Stats%20Web%20Scraping.ipynb)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3.2'></a>\n",
    "\n",
    "### <a id='#section3.2'>3.2. Outfield Players</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3.2.1'></a>\n",
    "\n",
    "#### <a id='#section3.2.1'>3.2.1. Data Dictionary</a>\n",
    "The raw dataset has one hundred and sixty four features (columns) with the following definitions and data types:\n",
    "\n",
    "| Variable     | Data Type    | Description    |\n",
    "|------|-----|-----|\n",
    "| `squad`    | object    | Squad name e.g. Arsenal    |\n",
    "| `players_used`    | float64    | Number of Players used in Games    |\n",
    "| `possession`    | float64    | Percentage of time with possession of the ball    |\n",
    "\n",
    "[MORE DEFINITIONS TO BE ADDED]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3.2.2'></a>\n",
    "\n",
    "#### <a id='#section3.2.2'>3.2.2. Creating the DataFrame - scraping the data</a>\n",
    "The data is scraped and saved as a DataFrame using the pandas `read_html()` function [[link](https://stackoverflow.com/questions/66517625/attributeerror-nonetype-object-has-no-attribute-text-beautifulshop)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping started at: 2021-11-03 13:21:11.712525\n",
      "Scraping started for the 2017-2018 season...\n",
      "Scraping started for player stats data for Big-5-European-Leagues league for the 2017-2018 season...\n",
      "Scraping Standard stats...\n",
      "Scraping Shooting stats...\n",
      "Scraping Passing stats...\n",
      "Scraping Pass Types stats...\n",
      "Scraping Goals and Shot Creation stats...\n",
      "Scraping Defensive Actions stats...\n",
      "Scraping Possession stats...\n",
      "Scraping Playing Time stats...\n",
      "Scraping Miscellaneous stats...\n",
      "All player stats data for the Big-5-European-Leagues league for 2017-2018 season scraped and saved.\n",
      "Scraping started for the 2018-2019 season...\n",
      "Scraping started for player stats data for Big-5-European-Leagues league for the 2018-2019 season...\n",
      "Scraping Standard stats...\n",
      "Scraping Shooting stats...\n",
      "Scraping Passing stats...\n",
      "Scraping Pass Types stats...\n",
      "Scraping Goals and Shot Creation stats...\n",
      "Scraping Defensive Actions stats...\n",
      "Scraping Possession stats...\n",
      "Scraping Playing Time stats...\n",
      "Scraping Miscellaneous stats...\n",
      "All player stats data for the Big-5-European-Leagues league for 2018-2019 season scraped and saved.\n",
      "Scraping started for the 2019-2020 season...\n",
      "Scraping started for player stats data for Big-5-European-Leagues league for the 2019-2020 season...\n",
      "Scraping Standard stats...\n",
      "Scraping Shooting stats...\n",
      "Scraping Passing stats...\n",
      "Scraping Pass Types stats...\n",
      "Scraping Goals and Shot Creation stats...\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 504: Gateway Time-out",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d7/wvbp_b411h75p3zvbmrvql080000gn/T/ipykernel_28792/592406383.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlst_seasons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'2017-2018'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2018-2019'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2019-2020'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2020-2021'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2021-2022'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_fbref_outfield_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fbref_player_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst_league_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlst_seasons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/d7/wvbp_b411h75p3zvbmrvql080000gn/T/ipykernel_28792/3548334672.py\u001b[0m in \u001b[0;36mget_fbref_player_stats\u001b[0;34m(lst_league_names, lst_seasons)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Scraping Goals and Shot Creation stats...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0murl_gca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fgca%2Fplayers%2F{season}-{league_name_long}&div=div_stats_gca'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mdf_gca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_gca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;31m##### Defensive Actions stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/html.py\u001b[0m in \u001b[0;36mread_html\u001b[0;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only)\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mkeep_default_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_default_na\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m         \u001b[0mdisplayed_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplayed_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m     )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/html.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, **kwargs)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m             \u001b[0mtables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcaught\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0;31m# if `io` is an io-like object, check if it's seekable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/html.py\u001b[0m in \u001b[0;36mparse_tables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mparsed\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfooter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mtuples\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mtables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_thead_tbody_tfoot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/html.py\u001b[0m in \u001b[0;36m_build_doc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    743\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text_content\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/html.py\u001b[0m in \u001b[0;36m_build_doc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m                     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 641\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 504: Gateway Time-out"
     ]
    }
   ],
   "source": [
    "lst_league_names = ['Big-5-European-Leagues']     #'Premier-League', 'Ligue-1', 'Bundesliga', 'Serie-A', 'La-Liga', 'Major-League-Soccer']\n",
    "lst_seasons = ['2017-2018', '2018-2019', '2019-2020', '2020-2021', '2021-2022']\n",
    "\n",
    "df_fbref_outfield_raw = get_fbref_player_stats(lst_league_names, lst_seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3.2.3'></a>\n",
    "\n",
    "#### <a id='#section3.2.3'>3.2.3. Preliminary Data Handling</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3.2.3.1'></a>\n",
    "\n",
    "##### <a id='#section3.2.3.1'>3.2.3.1. Summary Report</a>\n",
    "Initial step of the data handling and Exploratory Data Analysis (EDA) is to create a quick summary report of the dataset using [pandas Profiling Report](https://github.com/pandas-profiling/pandas-profiling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the data using pandas Profiling Report\n",
    "pp.ProfileReport(df_fbref_outfield_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3.2.3.2'></a>\n",
    "\n",
    "##### <a id='#section3.2.3.2'>3.2.3.2. Further Inspection</a>\n",
    "The following commands go into more bespoke summary of the dataset. Some of the commands include content covered in the [pandas Profiling](https://github.com/pandas-profiling/pandas-profiling) summary above, but using the standard [pandas](https://pandas.pydata.org/) functions and methods that most peoplem will be more familiar with.\n",
    "\n",
    "First check the quality of the dataset by looking first and last rows in pandas using the [head()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) and [tail()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first five rows of the raw DataFrame, df_fbref_outfield_raw\n",
    "df_fbref_outfield_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last five rows of the raw DataFrame, df_fbref_outfield_raw\n",
    "df_fbref_outfield_raw.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[shape](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html) returns a tuple representing the dimensionality of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the raw DataFrame, df_fbref_outfield_raw\n",
    "print(df_fbref_outfield_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[columns](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html) returns the column labels of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (column names) of the raw DataFrame, df_fbref_outfield_raw\n",
    "df_fbref_outfield_raw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [dtypes](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html) method returns the data types of each attribute in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays all columns\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(df_fbref_outfield_raw.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [info](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html) method to get a quick description of the data, in particular the total number of rows, and each attributeâ€™s type and number of non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info for the raw DataFrame, df_fbref_outfield_raw\n",
    "df_fbref_outfield_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [describe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) method to show some useful statistics for each numerical column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the raw DataFrame, df_fbref_outfield_raw, showing some summary statistics for each numberical column in the DataFrame\n",
    "df_fbref_outfield_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will check to see how many missing values we have i.e. the number of NULL values in the dataset, and in what features these missing values are located. This can be plotted nicely using the [missingno](https://pypi.org/project/missingno/) library (pip install missingno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot visualisation of the missing values for each feature of the raw DataFrame, df_fbref_outfield_raw\n",
    "msno.matrix(df_fbref_outfield_raw, figsize = (30, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts of missing values\n",
    "null_value_stats = df_fbref_outfield_raw.isnull().sum(axis=0)\n",
    "null_value_stats[null_value_stats != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualisation shows us very quickly that there are missing values in the dataset but as this data is scraped, this fine at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section3.4'>3.4. Goalkeepers</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section3.4.1'>3.4.1. Data Dictionary</a>\n",
    "The raw dataset has one hundred and eighty eight features (columns) with the following definitions and data types:\n",
    "\n",
    "| Variable     | Data Type    | Description    |\n",
    "|------|-----|-----|\n",
    "| `squad`    | object    | Squad name e.g. Arsenal    |\n",
    "| `players_used`    | float64    | Number of Players used in Games    |\n",
    "| `possession`    | float64    | Percentage of time with possession of the ball    |\n",
    "\n",
    "[MORE DEFINITIONS TO BE ADDED]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section3.4.2'>3.4.2. Creating the DataFrame - scraping the data</a>\n",
    "Scrape the data and save as a pandas DataFrame using the function `get_keeper_data`.\n",
    "\n",
    "Like the outfielders, to download the goalkeeper data we are not required to download the data for individual leagues and concatenate them, they can be downloaded as one from the 'Big 5' European leagues goalkeepers page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_league_names = ['Big-5-European-Leagues']     #'Premier-League', 'Ligue-1', 'Bundesliga', 'Serie-A', 'La-Liga', 'Major-League-Soccer']\n",
    "lst_seasons = ['2017-2018', '2018-2019', '2019-2020', '2020-2021', '2021-2022']\n",
    "\n",
    "df_fbref_goalkeeper_raw = get_fbref_goalkeeper_stats(lst_league_names, lst_seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section3.4.3'>3.4.3. Preliminary Data Handling</a>\n",
    "Let's quality of the dataset by looking first and last rows in pandas using the [head()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) and [tail()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first five rows of the raw DataFrame, df_fbref_goalkeeper_raw\n",
    "df_fbref_goalkeeper_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last five rows of the raw DataFrame, df_fbref_goalkeeper_raw\n",
    "df_fbref_goalkeeper_raw.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[shape](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html) returns a tuple representing the dimensionality of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the raw DataFrame, df_fbref_goalkeeper_raw\n",
    "print(df_fbref_goalkeeper_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[columns](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html) returns the column labels of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (column names) of the raw DataFrame, df_fbref_goalkeeper_raw\n",
    "df_fbref_goalkeeper_raw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [dtypes](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html) method returns the data types of each attribute in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays all columns\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(df_fbref_goalkeeper_raw.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [info](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html) method to get a quick description of the data, in particular the total number of rows, and each attributeâ€™s type and number of non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info for the raw DataFrame, df_fbref_goalkeeper_raw\n",
    "df_fbref_goalkeeper_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [describe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) method to show some useful statistics for each numerical column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the raw DataFrame, df_fbref_goalkeeper_raw, showing some summary statistics for each numberical column in the DataFrame\n",
    "df_fbref_goalkeeper_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will check to see how many missing values we have i.e. the number of NULL values in the dataset, and in what features these missing values are located. This can be plotted nicely using the [missingno](https://pypi.org/project/missingno/) library (pip install missingno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot visualisation of the missing values for each feature of the raw DataFrame, df_fbref_goalkeeper_raw\n",
    "msno.matrix(df_fbref_goalkeeper_raw, figsize = (30, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts of missing values\n",
    "null_value_stats = df_fbref_goalkeeper_raw.isnull().sum(axis=0)\n",
    "null_value_stats[null_value_stats != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualisation shows us very quickly that there are missing values in the dataset but as this data is scraped, this fine at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a id='section4'></a>\n",
    "\n",
    "## <a id='#section4'>4. Summary</a>\n",
    "This notebook scrapes player statstics data from [FBref](https://fbref.com/en/) via [StatsBomb](https://statsbomb.com/), using [pandas](http://pandas.pydata.org/) for data manipulation through DataFrames.\n",
    "\n",
    "With this notebook we now have aggregated player performance data for players in the 'Big 5' European leagues for the 17/18-present seasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a id='section5'></a>\n",
    "\n",
    "## <a id='#section5'>5. Next Steps</a>\n",
    "This data is now ready to be engineered before being matched to other datasets such as data from [TransferMarkt](https://www.transfermarkt.co.uk/) and [Capology](https://www.capology.com/).\n",
    "\n",
    "The Data Engineering subfolder in GitHub can be found [here](https://github.com/eddwebster/football_analytics/tree/master/notebooks/B\\)%20Data%20Engineering) and a static version of the FBref data engineering notebookecord can be found [here](https://nbviewer.org/github/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/FBref%20Player%20Stats%20Data%20Engineering.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a id='section6'></a>\n",
    "\n",
    "## <a id='#section6'>6. References</a>\n",
    "\n",
    "#### Data and Web Scraping\n",
    "*    [FBref](https://fbref.com/) for the data to scrape\n",
    "*    FBref statement for using StatsBomb's data: https://fbref.com/en/statsbomb/\n",
    "*    [StatsBomb](https://statsbomb.com/) providing the data to FBref\n",
    "*    [FBref_EPL GitHub repository](https://github.com/chmartin/FBref_EPL) by [chmartin](https://github.com/chmartin) for the original web scraping code\n",
    "*    [Scrape-FBref-data GitHub repository](https://github.com/parth1902/Scrape-FBref-data) by [parth1902](https://github.com/parth1902) for the revised web scraping code for the new FBref metrics\n",
    "\n",
    "\n",
    "#### Countries\n",
    "*    [Comparison of alphabetic country codes Wiki](https://en.wikipedia.org/wiki/Comparison_of_alphabetic_country_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "***Visit my website [eddwebster.com](https://www.eddwebster.com) or my [GitHub Repository](https://github.com/eddwebster) for more projects. If you'd like to get in contact, my Twitter handle is [@eddwebster](http://www.twitter.com/eddwebster) and my email is: edd.j.webster@gmail.com.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the top](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 642,
   "position": {
    "height": "664px",
    "left": "1119px",
    "right": "20px",
    "top": "-7px",
    "width": "489px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
