{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FBref Player Stats Web Scraping\n",
    "##### Notebook to scrape raw data from [FBref](https://fbref.com/en/) via [StatsBomb](https://statsbomb.com/) using the [pandas](http://pandas.pydata.org/)\n",
    "\n",
    "### By [Edd Webster](https://www.twitter.com/eddwebster)\n",
    "Notebook first written: 31/08/2020<br>\n",
    "Notebook last updated: 31/08/2021\n",
    "\n",
    "![title](../../img/fbref-logo-banner.png)\n",
    "\n",
    "![title](../../img/stats-bomb-logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a id='sectionintro'></a>\n",
    "\n",
    "## <a id='import_libraries'>Introduction</a>\n",
    "This notebook scrapes player statstics data from [FBref](https://fbref.com/en/) via [StatsBomb](https://statsbomb.com/), using the [pandas](http://pandas.pydata.org/) [`read_html`](https://pandas.pydata.org/docs/reference/api/pandas.read_html.html) function [pandas](http://pandas.pydata.org/) for webscraping and data manipulation through DataFrames.\n",
    "\n",
    "For more information about this notebook and the author, I am available through all the following channels:\n",
    "*    [eddwebster.com](https://www.eddwebster.com/);\n",
    "*    edd.j.webster@gmail.com;\n",
    "*    [@eddwebster](https://www.twitter.com/eddwebster);\n",
    "*    [linkedin.com/in/eddwebster](https://www.linkedin.com/in/eddwebster/);\n",
    "*    [github/eddwebster](https://github.com/eddwebster/);\n",
    "*    [public.tableau.com/profile/edd.webster](https://public.tableau.com/profile/edd.webster);\n",
    "*    [kaggle.com/eddwebster](https://www.kaggle.com/eddwebster); and\n",
    "*    [hackerrank.com/eddwebster](https://www.hackerrank.com/eddwebster).\n",
    "\n",
    "![title](../../img/fifa21eddwebsterbanner.png)\n",
    "\n",
    "The accompanying GitHub repository for this notebook can be found [here](https://github.com/eddwebster/football_analytics) and a static version of this notebook can be found [here](https://nbviewer.jupyter.org/github/eddwebster/football_analytics/blob/master/notebooks/A%29%20Web%20Scraping/FBref%20Web%20Scraping%20and%20Parsing.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a id='sectioncontents'></a>\n",
    "\n",
    "## <a id='notebook_contents'>Notebook Contents</a>\n",
    "1.    [Notebook Dependencies](#section1)<br>\n",
    "2.    [Project Brief](#section2)<br>\n",
    "3.    [Data Scraping](#section3)<br>\n",
    "      1.    [Introduction](#section3.1)<br>\n",
    "      2.    [Outfielder Players](#section3.2)<br>\n",
    "            1.    [Data Dictionary](#section3.2.1)<br>\n",
    "            2.    [Creating the DataFrame](#section3.2.2)<br>\n",
    "            3.    [Initial Data Handling](#section3.2.3)<br>\n",
    "            4.    [Export the Raw DataFrame](#section3.2.4)<br>\n",
    "      3.    [Goalkeepers](#section3.3)<br>\n",
    "            1.    [Data Dictionary](#section3.3.1)<br>\n",
    "            2.    [Creating the DataFrame](#section3.3.2)<br>\n",
    "            3.    [Initial Data Handling](#section3.3.3)<br>\n",
    "            4.    [Export the Raw DataFrame](#section3.3.4)<br> \n",
    "4.    [Summary](#section4)<br>\n",
    "5.    [Next Steps](#section5)<br>\n",
    "6.    [References](#section6)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a id='section1'></a>\n",
    "\n",
    "## <a id='#section1'>1. Notebook Dependencies</a>\n",
    "\n",
    "This notebook was written using [Python 3](https://docs.python.org/3.7/) and requires the following libraries:\n",
    "*    [`Jupyter notebooks`](https://jupyter.org/) for this notebook environment with which this project is presented;\n",
    "*    [`NumPy`](http://www.numpy.org/) for multidimensional array computing; and\n",
    "*    [`pandas`](http://pandas.pydata.org/) for data analysis and manipulation.\n",
    "\n",
    "All packages used for this notebook except for BeautifulSoup can be obtained by downloading and installing the [Conda](https://anaconda.org/anaconda/conda) distribution, available on all platforms (Windows, Linux and Mac OSX). Step-by-step guides on how to install Anaconda can be found for Windows [here](https://medium.com/@GalarnykMichael/install-python-on-windows-anaconda-c63c7c3d1444) and Mac [here](https://medium.com/@GalarnykMichael/install-python-on-mac-anaconda-ccd9f2014072), as well as in the Anaconda documentation itself [here](https://docs.anaconda.com/anaconda/install/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "# Python â‰¥3.5 (ideally)\n",
    "import platform\n",
    "import sys, getopt\n",
    "assert sys.version_info >= (3, 5)\n",
    "import csv\n",
    "\n",
    "# Import Dependencies\n",
    "%matplotlib inline\n",
    "\n",
    "# Math Operations\n",
    "import numpy as np\n",
    "from math import pi\n",
    "\n",
    "# Datetime\n",
    "import datetime\n",
    "from datetime import date\n",
    "import time\n",
    "\n",
    "# Data Preprocessing\n",
    "import pandas as pd\n",
    "#import pandas_profiling as pp\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import glob\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "# Reading directories\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Working with JSON\n",
    "import json\n",
    "\n",
    "# Web Scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Data Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Display in Jupyter\n",
    "from IPython.display import Image, YouTubeVideo\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "print('Setup Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.0\n",
      "NumPy: 1.26.2\n",
      "pandas: 2.1.3\n",
      "matplotlib: 3.8.2\n"
     ]
    }
   ],
   "source": [
    "# Python / module versions used here for reference\n",
    "print('Python: {}'.format(platform.python_version()))\n",
    "print('NumPy: {}'.format(np.__version__))\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('matplotlib: {}'.format(mpl.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up initial paths to subfolders\n",
    "base_dir = os.path.join('..', '..')\n",
    "data_dir = os.path.join(base_dir, 'data')\n",
    "data_dir_fbref = os.path.join(base_dir, 'data', 'fbref')\n",
    "img_dir = os.path.join(base_dir, 'img')\n",
    "fig_dir = os.path.join(base_dir, 'img', 'fig')\n",
    "video_dir = os.path.join(base_dir, 'video')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined variables\n",
    "\n",
    "## Define today's date\n",
    "today = datetime.datetime.now().strftime('%d/%m/%Y').replace('/', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define league names and their IDs\n",
    "dict_league_names = {'Premier-League': '9',\n",
    "                     'Ligue-1': '13',\n",
    "                     'Bundesliga': '20',\n",
    "                     'Serie-A': '11',\n",
    "                     'La-Liga': '12',\n",
    "                     'Major-League-Soccer': '22',\n",
    "                     'Big-5-European-Leagues': 'Big5'\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined Lists\n",
    "\n",
    "## Define list of long names for 'Big 5' European Leagues and MLS\n",
    "lst_league_names_long = ['Premier-League', 'Ligue-1', 'Bundesliga', 'Serie-A', 'La-Liga', 'Major-League-Soccer', 'Big-5-European-Leagues']\n",
    "\n",
    "## Define seasons to scrape\n",
    "lst_seasons = ['2017-2018', '2018-2019', '2019-2020', '2020-2021', '2021-2022','2022-2023']\n",
    "\n",
    "## Define list of folders\n",
    "lst_folders = ['raw', 'engineered', 'reference']\n",
    "\n",
    "## Define list of data types\n",
    "lst_data_types = ['goalkeeper', 'outfield', 'team']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Functions (Scrapers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for scraping a defined season and competition of FBref player data\n",
    "def get_fbref_player_stats(lst_league_names, lst_seasons):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to scrape player stats from FBref.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ## Define list of league names\n",
    "    league_names_long = lst_league_names\n",
    "    \n",
    "    \n",
    "    ## Define seasons to scrape\n",
    "    seasons = lst_seasons\n",
    "    \n",
    "    \n",
    "    ## Start timer\n",
    "    tic = datetime.datetime.now()\n",
    "    \n",
    "    \n",
    "    ## Print time scraping started\n",
    "    print(f'Scraping started at: {tic}')\n",
    "    \n",
    "    \n",
    "    ## Scrape information for each player\n",
    "    for season in seasons:\n",
    "\n",
    "        ### Print message\n",
    "        print(f'Scraping started for the {season} season...')\n",
    "\n",
    "        ### Loop through leagues\n",
    "        for league_name_long in league_names_long:\n",
    "            \n",
    "            #### Determine league short name from the league names dictionary\n",
    "            league_name_short = [v for k,v in dict_league_names.items() if k == league_name_long][0]\n",
    "            \n",
    "            #### Save Player URL List (if not already saved)\n",
    "            if not os.path.exists(os.path.join(data_dir_fbref + f'/raw/outfield/{league_name_long}/{season}/fbref_outfield_player_stats_{league_name_long}_{season}_latest.csv')):\n",
    "\n",
    "                ##### Scraping\n",
    "\n",
    "                ##### Print statement\n",
    "                print(f'Scraping started for player stats data for {league_name_long} league for the {season} season...')\n",
    "\n",
    "                ##### Standard stats\n",
    "                print(f'Scraping Standard stats...')\n",
    "                url_std_stats = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fstats%2Fplayers%2F{season}-{league_name_long}&div=div_stats_standard'\n",
    "                df_std_stats = pd.read_html(url_std_stats, header=1)[0]\n",
    "\n",
    "                ##### Goalkeeper stats\n",
    "                #print(f'Scraping Goalkeeper stats...')\n",
    "                #url_keepers = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fkeepers%2Fplayers%2F{season}-{league_name_long}&div=div_stats_keeper'\n",
    "                #df_keepers = pd.read_html(url_keepers, header=1)[0]\n",
    "\n",
    "                ##### Advanced Goalkeeper stats\n",
    "                #print(f'Scraping Advanced Goalkeeper stats...')\n",
    "                #url_keepers_adv = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fkeepersadv%2Fplayers%2F{season}-{league_name_long}&div=div_stats_keeper_adv'\n",
    "                #df_keepers_adv = pd.read_html(url_keepers_adv, header=1)[0]\n",
    "\n",
    "                ##### Shooting stats\n",
    "                print(f'Scraping Shooting stats...')\n",
    "                url_shooting = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fshooting%2Fplayers%2F{season}-{league_name_long}&div=div_stats_shooting'\n",
    "                df_shooting = pd.read_html(url_shooting, header=1)[0]\n",
    "\n",
    "                ##### Passing stats\n",
    "                print(f'Scraping Passing stats...')\n",
    "                url_passing = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fpassing%2Fplayers%2F{season}-{league_name_long}&div=div_stats_passing'\n",
    "                df_passing = pd.read_html(url_passing, header=1)[0]\n",
    "\n",
    "                ##### Pass Types stats\n",
    "                print(f'Scraping Pass Types stats...')\n",
    "                url_passing_types = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fpassing_types%2Fplayers%2F{season}-{league_name_long}&div=div_stats_passing_types'\n",
    "                df_passing_types = pd.read_html(url_passing_types, header=1)[0]\n",
    "\n",
    "                ##### Goals and Shot Creation stats\n",
    "                print(f'Scraping Goals and Shot Creation stats...')\n",
    "                url_gca = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fgca%2Fplayers%2F{season}-{league_name_long}&div=div_stats_gca'\n",
    "                df_gca = pd.read_html(url_gca, header=1)[0]\n",
    "\n",
    "                ##### Defensive Actions stats\n",
    "                print(f'Scraping Defensive Actions stats...')\n",
    "                url_defense = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fdefense%2Fplayers%2F{season}-{league_name_long}&div=div_stats_defense'\n",
    "                df_defense = pd.read_html(url_defense, header=1)[0]\n",
    "\n",
    "                ##### Possession stats\n",
    "                print(f'Scraping Possession stats...')\n",
    "                url_possession = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fpossession%2Fplayers%2F{season}-{league_name_long}&div=div_stats_possession'\n",
    "                df_possession = pd.read_html(url_possession, header=1)[0]\n",
    "\n",
    "                ##### Playing Time stats\n",
    "                print(f'Scraping Playing Time stats...')\n",
    "                url_playing_time = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fplayingtime%2Fplayers%2F{season}-{league_name_long}&div=div_stats_playing_time'\n",
    "                df_playing_time = pd.read_html(url_playing_time, header=1)[0]\n",
    "\n",
    "                ##### Miscellaneous stats\n",
    "                print(f'Scraping Miscellaneous stats...')\n",
    "                url_misc = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fmisc%2Fplayers%2F{season}-{league_name_long}&div=div_stats_misc'\n",
    "                df_misc = pd.read_html(url_misc, header=1)[0]\n",
    "\n",
    "                ##### Concatenate defined individual DataFrames\n",
    "                \n",
    "                ####### Define DataFrames to be concatenated side-by-side (not all of them)\n",
    "                lst_dfs = [df_std_stats, df_shooting, df_passing, df_passing_types, df_gca, df_defense, df_possession]\n",
    "\n",
    "                ###### Concatenate DataFrames side-by-side (indicated in list above)\n",
    "                df_all = pd.concat(lst_dfs, axis=1)\n",
    "\n",
    "                ###### Drop duplicate columns\n",
    "                df_all = df_all.loc[:,~df_all.columns.duplicated()]\n",
    "\n",
    "                ###### Drop duplicate rows\n",
    "                df_all = df_all.drop_duplicates()\n",
    "                \n",
    "                ##### Left join defined individual DataFrames\n",
    "                \n",
    "                ####### Define join conditions\n",
    "                conditions_join = ['Player', 'Nation', 'Pos', 'Squad', 'Comp']\n",
    "\n",
    "                ###### Left join Playing Time data\n",
    "                df_all = pd.merge(df_all, df_playing_time, left_on=conditions_join, right_on=conditions_join, how='left')\n",
    "\n",
    "                ###### Remove duplicate columns after join (contain '_y') and remove '_x' suffix from kept columns\n",
    "                df_all = df_all[df_all.columns.drop(list(df_all.filter(regex='_y')))]\n",
    "                df_all.columns = df_all.columns.str.replace('_x','')\n",
    "                \n",
    "                ###### Drop duplicate rows\n",
    "                df_all = df_all.drop_duplicates()\n",
    "\n",
    "                ###### Left join Misc data\n",
    "                df_all = pd.merge(df_all, df_misc, left_on=conditions_join, right_on=conditions_join, how='left')\n",
    "\n",
    "                ###### Remove duplicate columns after join (contain '_y') and remove '_x' suffix from kept columns\n",
    "                df_all = df_all[df_all.columns.drop(list(df_all.filter(regex='_y')))]\n",
    "                df_all.columns = df_all.columns.str.replace('_x','')\n",
    "                \n",
    "                ###### Drop duplicate rows\n",
    "                df_all = df_all.drop_duplicates()\n",
    "                \n",
    "                \n",
    "                ##### Engineer DataFrames\n",
    "                \n",
    "                ###### Take first two digits of age - fixes current season issue with extra values\n",
    "                df_all['Age'] = df_all['Age'].astype(str).str[:2]\n",
    "                \n",
    "                ###### Create columns for league code and season\n",
    "                df_all['League Name'] = league_name_long\n",
    "                df_all['League ID'] = league_name_short\n",
    "                df_all['Season'] = season              \n",
    "\n",
    "                ###### Drop duplicates\n",
    "                df_all = df_all.drop_duplicates()\n",
    "\n",
    "                \n",
    "                ##### Save DataFrame\n",
    "                df_all.to_csv(data_dir_fbref + f'/raw/outfield/{league_name_long}/{season}/fbref_outfield_player_stats_{league_name_long}_{season}_latest.csv', index=None, header=True, encoding='utf-8')        \n",
    "                \n",
    "                ##### Export a copy to the 'archive' subfolder, including the date\n",
    "                df_all.to_csv(data_dir_fbref + f'/raw/outfield/{league_name_long}/{season}/archive/fbref_outfield_player_stats_{league_name_long}_{season}_last_updated_{today}.csv', index=None, header=True, encoding='utf-8')        \n",
    "                \n",
    "                \n",
    "                ##### Print statement for league and season\n",
    "                print(f'All player stats data for the {league_name_long} league for {season} season scraped and saved.')\n",
    "             \n",
    "            \n",
    "            #### Load player stats data (if already saved)\n",
    "            else:\n",
    "\n",
    "                ##### Print statement\n",
    "                print(f'Player stats data for the {league_name_long} league for the {season} season already saved as a CSV file.')         \n",
    "\n",
    "                \n",
    "    ## End timer\n",
    "    toc = datetime.datetime.now()\n",
    "    \n",
    "    \n",
    "    ## Print time scraping ended\n",
    "    print(f'Scraping ended at: {toc}')\n",
    "\n",
    "    \n",
    "    ## Calculate time take\n",
    "    total_time = (toc-tic).total_seconds()\n",
    "    print(f'Time taken to scrape the player stats data for {len(league_names_long)} leagues for {len(seasons)} seasons is: {total_time/60:0.2f} minutes.')\n",
    "\n",
    "    \n",
    "    ## Unify individual CSV files as a single DataFrame\n",
    "    \n",
    "    ### Show files in directory\n",
    "    all_files = glob.glob(os.path.join(data_dir_fbref + f'/raw/outfield/*/*/fbref_outfield_player_stats_*_*_latest.csv'))\n",
    "    \n",
    "    ### Create an empty list of Players URLs\n",
    "    lst_player_stats_all = []\n",
    "\n",
    "    ### Loop through list of files and read into temporary DataFrames\n",
    "    for filename in all_files:\n",
    "        df_temp = pd.read_csv(filename, index_col=None, header=0)\n",
    "        lst_player_stats_all.append(df_temp)\n",
    "\n",
    "    ### Concatenate the files into a single DataFrame\n",
    "    df_fbref_player_stats_all = pd.concat(lst_player_stats_all, axis=0, ignore_index=True)\n",
    "    \n",
    "    ### Drop header row of each concatenated  DataFrame (contains 'Rk', 'Rk' column)\n",
    "    df_fbref_player_stats_all = df_fbref_player_stats_all[~df_fbref_player_stats_all['Rk'].str.contains('Rk')]\n",
    "    \n",
    "    ### Drop 'Rk' column\n",
    "    df_fbref_player_stats_all = df_fbref_player_stats_all.drop(['Rk'], axis=1)\n",
    "    \n",
    "    ### Reset index\n",
    "    #df_fbref_player_stats_all = df_fbref_player_stats_all.reset_index()\n",
    "    \n",
    "    ### Sort DataFrame\n",
    "    df_fbref_player_stats_all = df_fbref_player_stats_all.sort_values(['League Name', 'Season', 'Player'], ascending=[True, True, True])\n",
    "\n",
    "    \n",
    "    ## Export DataFrame\n",
    "    \n",
    "    ###\n",
    "    df_fbref_player_stats_all.to_csv(data_dir_fbref + f'/raw/outfield/fbref_outfield_player_stats_combined_latest.csv', index=None, header=True, encoding='utf-8')\n",
    "    \n",
    "    ### Save a copy to archive folder (dated)\n",
    "    df_fbref_player_stats_all.to_csv(data_dir_fbref + f'/raw/outfield/archive/fbref_outfield_player_stats_combined_last_updated_{today}.csv', index=None, header=True, encoding='utf-8')\n",
    "    \n",
    "    \n",
    "    ## Distinct number of players\n",
    "    total_players = df_fbref_player_stats_all['Player'].nunique()\n",
    "\n",
    "\n",
    "    ## Print statement\n",
    "    print(f'Player stats DataFrame contains {total_players} players.')\n",
    "    \n",
    "    \n",
    "    ## Return final list of Player URLs\n",
    "    return(df_fbref_player_stats_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for scraping a defined season and competition of FBref player data\n",
    "def get_fbref_goalkeeper_stats(lst_league_names, lst_seasons):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to scrape goalkeeper stats from FBref.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ## Define list of league names\n",
    "    league_names_long = lst_league_names\n",
    "    \n",
    "    \n",
    "    ## Define seasons to scrape\n",
    "    seasons = lst_seasons\n",
    "    \n",
    "    \n",
    "    ## Start timer\n",
    "    tic = datetime.datetime.now()\n",
    "    \n",
    "    \n",
    "    ## Print time scraping started\n",
    "    print(f'Scraping started at: {tic}')\n",
    "    \n",
    "    \n",
    "    ## Scrape information for each player\n",
    "    for season in seasons:\n",
    "\n",
    "        ### Print message\n",
    "        print(f'Scraping started for the {season} season...')\n",
    "\n",
    "        ### Loop through leagues\n",
    "        for league_name_long in league_names_long:\n",
    "            \n",
    "            #### Determine league short name from the league names dictionary\n",
    "            league_name_short = [v for k,v in dict_league_names.items() if k == league_name_long][0]\n",
    "            \n",
    "            #### Save Player URL List (if not already saved)\n",
    "            if not os.path.exists(os.path.join(data_dir_fbref + f'/raw/goalkeeper/{league_name_long}/{season}/fbref_goalkeeper_stats_{league_name_long}_{season}_latest.csv', encoding='utf-8')):\n",
    "\n",
    "                ##### Scraping\n",
    "\n",
    "                ##### Print statement\n",
    "                print(f'Scraping started for goalkeeper stats data for {league_name_long} league for the {season} season...')\n",
    "\n",
    "                ##### Standard stats\n",
    "                print(f'Scraping Standard stats...')\n",
    "                url_std_stats = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fstats%2Fplayers%2F{season}-{league_name_long}&div=div_stats_standard'\n",
    "                df_std_stats = pd.read_html(url_std_stats, header=1)[0]\n",
    "\n",
    "                ##### Goalkeeper stats\n",
    "                print(f'Scraping Goalkeeper stats...')\n",
    "                url_keepers = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fkeepers%2Fplayers%2F{season}-{league_name_long}&div=div_stats_keeper'\n",
    "                df_keepers = pd.read_html(url_keepers, header=1)[0]\n",
    "\n",
    "                ##### Advanced Goalkeeper stats\n",
    "                print(f'Scraping Advanced Goalkeeper stats...')\n",
    "                url_keepers_adv = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fkeepersadv%2Fplayers%2F{season}-{league_name_long}&div=div_stats_keeper_adv'\n",
    "                df_keepers_adv = pd.read_html(url_keepers_adv, header=1)[0]\n",
    "\n",
    "                ##### Playing Time stats\n",
    "                print(f'Scraping Playing Time stats...')\n",
    "                url_playing_time = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fplayingtime%2Fplayers%2F{season}-{league_name_long}&div=div_stats_playing_time'\n",
    "                df_playing_time = pd.read_html(url_playing_time, header=1)[0]\n",
    "\n",
    "                ##### Miscellaneous stats\n",
    "                print(f'Scraping Miscellaneous stats...')\n",
    "                url_misc = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fmisc%2Fplayers%2F{season}-{league_name_long}&div=div_stats_misc'\n",
    "                df_misc = pd.read_html(url_misc, header=1)[0]\n",
    "\n",
    "                ##### Concatenate defined individual DataFrames\n",
    "                \n",
    "                ####### Define DataFrames to be concatenated side-by-side (not all of them)\n",
    "                lst_dfs = [df_keepers, df_keepers_adv]\n",
    "\n",
    "                ###### Concatenate DataFrames side-by-side (indicated in list above)\n",
    "                df_all = pd.concat(lst_dfs, axis=1)\n",
    "\n",
    "                ###### Drop duplicate columns\n",
    "                df_all = df_all.loc[:,~df_all.columns.duplicated()]\n",
    "\n",
    "                ###### Drop duplicate rows\n",
    "                df_all = df_all.drop_duplicates()\n",
    "                \n",
    "                ##### Left join defined individual DataFrames\n",
    "                \n",
    "                ####### Define join conditions\n",
    "                conditions_join = ['Player', 'Nation', 'Pos', 'Squad', 'Comp']\n",
    "\n",
    "                ###### Left join Standard Stats data\n",
    "                df_all = pd.merge(df_all, df_std_stats, left_on=conditions_join, right_on=conditions_join, how='left')\n",
    "\n",
    "                ###### Remove duplicate columns after join (contain '_y') and remove '_x' suffix from kept columns\n",
    "                df_all = df_all[df_all.columns.drop(list(df_all.filter(regex='_y')))]\n",
    "                df_all.columns = df_all.columns.str.replace('_x','')\n",
    "                \n",
    "                ###### Drop duplicate rows\n",
    "                df_all = df_all.drop_duplicates()\n",
    "                \n",
    "                ###### Left join Playing Time data\n",
    "                df_all = pd.merge(df_all, df_playing_time, left_on=conditions_join, right_on=conditions_join, how='left')\n",
    "\n",
    "                ###### Remove duplicate columns after join (contain '_y') and remove '_x' suffix from kept columns\n",
    "                df_all = df_all[df_all.columns.drop(list(df_all.filter(regex='_y')))]\n",
    "                df_all.columns = df_all.columns.str.replace('_x','')\n",
    "                \n",
    "                ###### Drop duplicate rows\n",
    "                df_all = df_all.drop_duplicates()\n",
    "\n",
    "                ###### Left join Misc data\n",
    "                df_all = pd.merge(df_all, df_misc, left_on=conditions_join, right_on=conditions_join, how='left')\n",
    "\n",
    "                ###### Remove duplicate columns after join (contain '_y') and remove '_x' suffix from kept columns\n",
    "                df_all = df_all[df_all.columns.drop(list(df_all.filter(regex='_y')))]\n",
    "                df_all.columns = df_all.columns.str.replace('_x','')\n",
    "                \n",
    "                ###### Drop duplicate rows\n",
    "                df_all = df_all.drop_duplicates()\n",
    "                \n",
    "                \n",
    "                ##### Engineer DataFrames\n",
    "                \n",
    "                ###### Take first two digits of age - fixes current season issue with extra values\n",
    "                df_all['Age'] = df_all['Age'].astype(str).str[:2]\n",
    "                \n",
    "                ###### Create columns for league code and season\n",
    "                df_all['League Name'] = league_name_long\n",
    "                df_all['League ID'] = league_name_short\n",
    "                df_all['Season'] = season              \n",
    "\n",
    "                ###### Drop duplicates\n",
    "                df_all = df_all.drop_duplicates()\n",
    "\n",
    "                \n",
    "                ##### Save DataFrame\n",
    "                df_all.to_csv(data_dir_fbref + f'/raw/goalkeeper/{league_name_long}/{season}/fbref_goalkeeper_stats_{league_name_long}_{season}_latest.csv', index=None, header=True, encoding='utf-8')        \n",
    "                \n",
    "                ##### Export a copy to the 'archive' subfolder, including the date\n",
    "                df_all.to_csv(data_dir_fbref + f'/raw/goalkeeper/{league_name_long}/{season}/archive/fbref_goalkeeper_stats_{league_name_long}_{season}_last_updated_{today}.csv', index=None, header=True, encoding='utf-8')        \n",
    "                \n",
    "                \n",
    "                ##### Print statement for league and season\n",
    "                print(f'All Goalkeeper stats data for the {league_name_long} league for {season} season scraped and saved.')\n",
    "             \n",
    "            \n",
    "            #### Load goalkeeper stats data (if already saved)\n",
    "            else:\n",
    "\n",
    "                ##### Print statement\n",
    "                print(f'Goalkeeper stats data for the {league_name_long} league for the {season} season already saved as a CSV file.')         \n",
    "\n",
    "                \n",
    "    ## End timer\n",
    "    toc = datetime.datetime.now()\n",
    "    \n",
    "    \n",
    "    ## Print time scraping ended\n",
    "    print(f'Scraping ended at: {toc}')\n",
    "\n",
    "    \n",
    "    ## Calculate time take\n",
    "    total_time = (toc-tic).total_seconds()\n",
    "    print(f'Time taken to scrape the goalkeeper stats data for {len(league_names_long)} leagues for {len(seasons)} seasons is: {total_time/60:0.2f} minutes.')\n",
    "\n",
    "    \n",
    "    ## Unify individual CSV files as a single DataFrame\n",
    "    \n",
    "    ### Show files in directory\n",
    "    all_files = glob.glob(os.path.join(data_dir_fbref + f'/raw/goalkeeper/*/*/fbref_goalkeeper_stats_*_*_latest.csv'))\n",
    "    \n",
    "    ### Create an empty list of Players URLs\n",
    "    lst_goalkeeper_stats_all = []\n",
    "\n",
    "    ### Loop through list of files and read into temporary DataFrames\n",
    "    for filename in all_files:\n",
    "        df_temp = pd.read_csv(filename, index_col=None, header=0)\n",
    "        lst_goalkeeper_stats_all.append(df_temp)\n",
    "\n",
    "    ### Concatenate the files into a single DataFrame\n",
    "    df_fbref_goalkeeper_stats_all = pd.concat(lst_goalkeeper_stats_all, axis=0, ignore_index=True)\n",
    "    \n",
    "    ### Drop header row of each concatenated  DataFrame (contains 'Rk', 'Rk' column)\n",
    "    df_fbref_goalkeeper_stats_all = df_fbref_goalkeeper_stats_all[~df_fbref_goalkeeper_stats_all['Rk'].str.contains('Rk')]\n",
    "    \n",
    "    ### Drop 'Rk' column\n",
    "    df_fbref_goalkeeper_stats_all = df_fbref_goalkeeper_stats_all.drop(['Rk'], axis=1)\n",
    "    \n",
    "    ### Reset index\n",
    "    #df_fbref_goalkeeper_stats_all = df_fbref_goalkeeper_stats_all.reset_index()\n",
    "    \n",
    "    ### Sort DataFrame\n",
    "    df_fbref_goalkeeper_stats_all = df_fbref_goalkeeper_stats_all.sort_values(['League Name', 'Season', 'Player'], ascending=[True, True, True])\n",
    "\n",
    "    \n",
    "    ## Export DataFrame\n",
    "    \n",
    "    ###\n",
    "    df_fbref_goalkeeper_stats_all.to_csv(data_dir_fbref + f'/raw/goalkeeper/fbref_goalkeeper_stats_combined_latest.csv', index=None, header=True, encoding='utf-8')\n",
    "    \n",
    "    ### Save a copy to archive folder (dated)\n",
    "    df_fbref_goalkeeper_stats_all.to_csv(data_dir_fbref + f'/raw/goalkeeper/archive/fbref_goalkeeper_stats_combined_last_updated_{today}.csv', index=None, header=True, encoding='utf-8')\n",
    "    \n",
    "    \n",
    "    ## Distinct number of goalkeepers\n",
    "    total_players = df_fbref_goalkeeper_stats_all['Player'].nunique()\n",
    "\n",
    "\n",
    "    ## Print statement\n",
    "    print(f'Goalkeeper stats DataFrame contains {total_players} players.')\n",
    "    \n",
    "    \n",
    "    ## Return final list of Player URLs\n",
    "    return(df_fbref_goalkeeper_stats_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Directory Structure\n",
    "Create folders and subfolders for data, if not already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '..\\\\..\\\\data\\\\fbref\\\\raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir_fbref, folder)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_types \u001b[38;5;129;01min\u001b[39;00m lst_data_types:\n\u001b[0;32m      7\u001b[0m         path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir_fbref, folder, data_types)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '..\\\\..\\\\data\\\\fbref\\\\raw'"
     ]
    }
   ],
   "source": [
    "# Make the data directory structure\n",
    "for folder in lst_folders:\n",
    "    path = os.path.join(data_dir_fbref, folder)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "        for data_types in lst_data_types:\n",
    "            path = os.path.join(data_dir_fbref, folder, data_types)\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "                os.mkdir(os.path.join(path, 'archive'))\n",
    "                for league in lst_league_names_long:\n",
    "                    path = os.path.join(data_dir_fbref, folder, data_types, league)\n",
    "                    if not os.path.exists(path):\n",
    "                        os.mkdir(path)\n",
    "                        for season in lst_seasons:\n",
    "                            path = os.path.join(data_dir_fbref, folder, data_types, league, season)\n",
    "                            if not os.path.exists(path):\n",
    "                                os.mkdir(path)\n",
    "                                os.mkdir(os.path.join(path, 'archive'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all columns of pandas DataFrames\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section2'></a>\n",
    "\n",
    "## <a id='#section2'>2. Project Brief</a>\n",
    "This Jupyter notebook is part of a series of notebooks, to scrape, parse, engineer, and unify datasets, that can be used for modeling purposes.\n",
    "\n",
    "This particular notebook is one of several web scraping notebooks, that takes data from [FBref](https://fbref.com/en/), provided by [StatsBomb](https://statsbomb.com/), and scrapes it using the [pandas](http://pandas.pydata.org/) [`read_html`](https://pandas.pydata.org/docs/reference/api/pandas.read_html.html) function and manipulates it as Dataframe.\n",
    "\n",
    "This notebook, along with the other notebooks in this project workflow are shown in the following diagram:\n",
    "\n",
    "![roadmap](../../img/football_analytics_data_roadmap.png)\n",
    "\n",
    "Links to these notebooks in the [`football_analytics`](https://github.com/eddwebster/football_analytics) GitHub repository can be found at the following:\n",
    "*    [1. Webscraping](https://github.com/eddwebster/football_analytics/tree/master/notebooks/1_data_scraping)\n",
    "     +    [FBref Player Stats Webscraping](https://github.com/eddwebster/football_analytics/blob/master/notebooks/1_data_scraping/FBref%20Player%20Stats%20Web%20Scraping.ipynb)\n",
    "     +    [TransferMarket Player Bio and Status Webscraping](https://github.com/eddwebster/football_analytics/blob/master/notebooks/1_data_scraping/TransferMarkt%20Player%20Bio%20and%20Status%20Web%20Scraping.ipynb)\n",
    "     +    [TransferMarket Player Valuation Webscraping](https://github.com/eddwebster/football_analytics/blob/master/notebooks/1_data_scraping/TransferMarkt%20Player%20Valuation%20Web%20Scraping.ipynb)\n",
    "     +    [TransferMarkt Player Recorded Transfer Fees Webscraping](https://github.com/eddwebster/football_analytics/blob/master/notebooks/1_data_scraping/TransferMarkt%20Player%20Recorded%20Transfer%20Fees%20Webscraping.ipynb)\n",
    "     +    [Capology Player Salary Webscraping](https://github.com/eddwebster/football_analytics/blob/master/notebooks/1_data_scraping/Capology%20Player%20Salary%20Web%20Scraping.ipynb)\n",
    "     +    [FBref Team Stats Webscraping](https://github.com/eddwebster/football_analytics/blob/master/notebooks/1_data_scraping/FBref%20Team%20Stats%20Web%20Scraping.ipynb)\n",
    "*    [2. Data Parsing](https://github.com/eddwebster/football_analytics/tree/master/notebooks/2_data_parsing)\n",
    "     +    [ELO Team Ratings Data Parsing](https://github.com/eddwebster/football_analytics/blob/master/notebooks/2_data_parsing/ELO%20Team%20Ratings%20Data%20Parsing.ipynb)\n",
    "*    [3. Data Engineering](https://github.com/eddwebster/football_analytics/tree/master/notebooks/3_data_engineering)\n",
    "     +    [FBref Player Stats Data Engineering](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/FBref%20Player%20Stats%20Data%20Engineering.ipynb)\n",
    "     +    [TransferMarket Player Bio and Status Data Engineering](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/TransferMarkt%20Player%20Bio%20and%20Status%20Data%20Engineering.ipynb)\n",
    "     +    [TransferMarket Player Valuation Data Engineering](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/TransferMarkt%20Player%20Valuation%20Data%20Engineering.ipynb)\n",
    "     +    [TransferMarkt Player Recorded Transfer Fees Data Engineering](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/TransferMarkt%20Player%20Recorded%20Transfer%20Fees%20Data%20Engineering.ipynb)\n",
    "     +    [Capology Player Salary Data Engineering](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/Capology%20Player%20Salary%20Data%20Engineering.ipynb)\n",
    "     +    [FBref Team Stats Data Engineering](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/FBref%20Team%20Stats%20Data%20Engineering.ipynb)\n",
    "     +    [ELO Team Ratings Data Parsing](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/ELO%20Team%20Ratings%20Data%20Parsing.ipynb)\n",
    "     +    [TransferMarkt Team Recorded Transfer Fee Data Engineering](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/TransferMarkt%20Team%20Recorded%20Transfer%20Fee%20Data%20Engineering.ipynb) (aggregated from [TransferMarkt Player Recorded Transfer Fees notebook](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/TransferMarkt%20Player%20Recorded%20Transfer%20Fees%20Data%20Engineering.ipynb))\n",
    "     +    [Capology Team Salary Data Engineering](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/Capology%20Team%20Salary%20Data%20Engineering.ipynb) (aggregated from [Capology Player Salary notebook](https://github.com/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/Capology%20Player%20Salary%20Data%20Engineering.ipynb))\n",
    "*    [4. Data Unification](https://github.com/eddwebster/football_analytics/tree/master/notebooks/4_data_unification)\n",
    "*    [5. Modeling and Data Analysis]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section3'></a>\n",
    "\n",
    "## <a id='#section3'>3. Data Scraping</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3.1'></a>\n",
    "\n",
    "### <a id='#section3.1'>3.1. Introduction</a>\n",
    "This Data Sources section has been has been split into two subsections - outfielder, and goalkeeper.\n",
    "\n",
    "The data needs to be scraped, converted to a pandas DataFrame ([Section 3](#section3)) and cleaned in the Data Engineering section ([Section 4](#section4)).\n",
    "\n",
    "We'll be using the [pandas](http://pandas.pydata.org/) library to import our data to this workbook as a DataFrame.\n",
    "\n",
    "From FBref it is also possible to scrape team data, this is covered separately in the following notebook in the [Data Scraping](https://github.com/eddwebster/football_analytics/tree/master/notebooks/1_data_scraping) folder [[link](https://github.com/eddwebster/football_analytics/blob/master/notebooks/1_data_scraping/FBref%20Team%20Stats%20Web%20Scraping.ipynb)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3.2'></a>\n",
    "\n",
    "### <a id='#section3.2'>3.2. Outfield Players</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3.2.1'></a>\n",
    "\n",
    "#### <a id='#section3.2.1'>3.2.1. Data Dictionary</a>\n",
    "The raw dataset has one hundred and sixty four features (columns) with the following definitions and data types:\n",
    "\n",
    "| Variable     | Data Type    | Description    |\n",
    "|------|-----|-----|\n",
    "| `squad`    | object    | Squad name e.g. Arsenal    |\n",
    "| `players_used`    | float64    | Number of Players used in Games    |\n",
    "| `possession`    | float64    | Percentage of time with possession of the ball    |\n",
    "\n",
    "[MORE DEFINITIONS TO BE ADDED]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3.2.2'></a>\n",
    "\n",
    "#### <a id='#section3.2.2'>3.2.2. Creating the DataFrame - scraping the data</a>\n",
    "The data is scraped and saved as a DataFrame using the pandas `read_html()` function [[link](https://stackoverflow.com/questions/66517625/attributeerror-nonetype-object-has-no-attribute-text-beautifulshop)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping started at: 2024-01-04 00:08:14.217596\n",
      "Scraping started for the 2017-2018 season...\n",
      "Scraping started for player stats data for Big-5-European-Leagues league for the 2017-2018 season...\n",
      "Scraping Standard stats...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'lxml'.  Use pip or conda to install lxml.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\compat\\_optional.py:132\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 132\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1381\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1354\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1304\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1381\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1354\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1318\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lxml'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m lst_league_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBig-5-European-Leagues\u001b[39m\u001b[38;5;124m'\u001b[39m]     \u001b[38;5;66;03m#'Premier-League', 'Ligue-1', 'Bundesliga', 'Serie-A', 'La-Liga', 'Major-League-Soccer']\u001b[39;00m\n\u001b[0;32m      2\u001b[0m lst_seasons \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2017-2018\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2018-2019\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2019-2020\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2020-2021\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2021-2022\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2022-2023\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m df_fbref_outfield_raw \u001b[38;5;241m=\u001b[39m \u001b[43mget_fbref_player_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlst_league_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlst_seasons\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 48\u001b[0m, in \u001b[0;36mget_fbref_player_stats\u001b[1;34m(lst_league_names, lst_seasons)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScraping Standard stats...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     47\u001b[0m url_std_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mleague_name_short\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%2F\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%2Fstats%2Fplayers%2F\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mleague_name_long\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&div=div_stats_standard\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 48\u001b[0m df_std_stats \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_std_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m##### Goalkeeper stats\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m#print(f'Scraping Goalkeeper stats...')\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m#url_keepers = f'https://widgets.sports-reference.com/wg.fcgi?css=1&site=fb&url=%2Fen%2Fcomps%2F{league_name_short}%2F{season}%2Fkeepers%2Fplayers%2F{season}-{league_name_long}&div=div_stats_keeper'\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m##### Shooting stats\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScraping Shooting stats...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\html.py:1245\u001b[0m, in \u001b[0;36mread_html\u001b[1;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\u001b[0m\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m   1230\u001b[0m     [\n\u001b[0;32m   1231\u001b[0m         is_file_like(io),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1235\u001b[0m     ]\n\u001b[0;32m   1236\u001b[0m ):\n\u001b[0;32m   1237\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal html to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_html\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1242\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1243\u001b[0m     )\n\u001b[1;32m-> 1245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\html.py:976\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    974\u001b[0m retained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m flav \u001b[38;5;129;01min\u001b[39;00m flavor:\n\u001b[1;32m--> 976\u001b[0m     parser \u001b[38;5;241m=\u001b[39m \u001b[43m_parser_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflav\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m     p \u001b[38;5;241m=\u001b[39m parser(\n\u001b[0;32m    978\u001b[0m         io,\n\u001b[0;32m    979\u001b[0m         compiled_match,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    984\u001b[0m         storage_options,\n\u001b[0;32m    985\u001b[0m     )\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\html.py:923\u001b[0m, in \u001b[0;36m_parser_dispatch\u001b[1;34m(flavor)\u001b[0m\n\u001b[0;32m    921\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbs4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 923\u001b[0m     \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlxml.etree\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _valid_parsers[flavor]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\compat\\_optional.py:135\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 135\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'lxml'.  Use pip or conda to install lxml."
     ]
    }
   ],
   "source": [
    "lst_league_names = ['Big-5-European-Leagues']     #'Premier-League', 'Ligue-1', 'Bundesliga', 'Serie-A', 'La-Liga', 'Major-League-Soccer']\n",
    "lst_seasons = ['2017-2018', '2018-2019', '2019-2020', '2020-2021', '2021-2022','2022-2023']\n",
    "\n",
    "df_fbref_outfield_raw = get_fbref_player_stats(lst_league_names, lst_seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3.2.3'></a>\n",
    "\n",
    "#### <a id='#section3.2.3'>3.2.3. Preliminary Data Handling</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3.2.3.1'></a>\n",
    "\n",
    "##### <a id='#section3.2.3.1'>3.2.3.1. Summary Report</a>\n",
    "Initial step of the data handling and Exploratory Data Analysis (EDA) is to create a quick summary report of the dataset using [pandas Profiling Report](https://github.com/pandas-profiling/pandas-profiling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the data using pandas Profiling Report\n",
    "pp.ProfileReport(df_fbref_outfield_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3.2.3.2'></a>\n",
    "\n",
    "##### <a id='#section3.2.3.2'>3.2.3.2. Further Inspection</a>\n",
    "The following commands go into more bespoke summary of the dataset. Some of the commands include content covered in the [pandas Profiling](https://github.com/pandas-profiling/pandas-profiling) summary above, but using the standard [pandas](https://pandas.pydata.org/) functions and methods that most peoplem will be more familiar with.\n",
    "\n",
    "First check the quality of the dataset by looking first and last rows in pandas using the [head()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) and [tail()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first five rows of the raw DataFrame, df_fbref_outfield_raw\n",
    "df_fbref_outfield_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last five rows of the raw DataFrame, df_fbref_outfield_raw\n",
    "df_fbref_outfield_raw.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[shape](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html) returns a tuple representing the dimensionality of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the raw DataFrame, df_fbref_outfield_raw\n",
    "print(df_fbref_outfield_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[columns](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html) returns the column labels of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (column names) of the raw DataFrame, df_fbref_outfield_raw\n",
    "df_fbref_outfield_raw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [dtypes](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html) method returns the data types of each attribute in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays all columns\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(df_fbref_outfield_raw.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [info](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html) method to get a quick description of the data, in particular the total number of rows, and each attributeâ€™s type and number of non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info for the raw DataFrame, df_fbref_outfield_raw\n",
    "df_fbref_outfield_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [describe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) method to show some useful statistics for each numerical column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the raw DataFrame, df_fbref_outfield_raw, showing some summary statistics for each numberical column in the DataFrame\n",
    "df_fbref_outfield_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will check to see how many missing values we have i.e. the number of NULL values in the dataset, and in what features these missing values are located. This can be plotted nicely using the [missingno](https://pypi.org/project/missingno/) library (pip install missingno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot visualisation of the missing values for each feature of the raw DataFrame, df_fbref_outfield_raw\n",
    "msno.matrix(df_fbref_outfield_raw, figsize = (30, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts of missing values\n",
    "null_value_stats = df_fbref_outfield_raw.isnull().sum(axis=0)\n",
    "null_value_stats[null_value_stats != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualisation shows us very quickly that there are missing values in the dataset but as this data is scraped, this fine at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section3.4'>3.4. Goalkeepers</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section3.4.1'>3.4.1. Data Dictionary</a>\n",
    "The raw dataset has one hundred and eighty eight features (columns) with the following definitions and data types:\n",
    "\n",
    "| Variable     | Data Type    | Description    |\n",
    "|------|-----|-----|\n",
    "| `squad`    | object    | Squad name e.g. Arsenal    |\n",
    "| `players_used`    | float64    | Number of Players used in Games    |\n",
    "| `possession`    | float64    | Percentage of time with possession of the ball    |\n",
    "\n",
    "[MORE DEFINITIONS TO BE ADDED]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section3.4.2'>3.4.2. Creating the DataFrame - scraping the data</a>\n",
    "Scrape the data and save as a pandas DataFrame using the function `get_keeper_data`.\n",
    "\n",
    "Like the outfielders, to download the goalkeeper data we are not required to download the data for individual leagues and concatenate them, they can be downloaded as one from the 'Big 5' European leagues goalkeepers page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_league_names = ['Big-5-European-Leagues']     #'Premier-League', 'Ligue-1', 'Bundesliga', 'Serie-A', 'La-Liga', 'Major-League-Soccer']\n",
    "lst_seasons = ['2017-2018', '2018-2019', '2019-2020', '2020-2021', '2021-2022']\n",
    "\n",
    "df_fbref_goalkeeper_raw = get_fbref_goalkeeper_stats(lst_league_names, lst_seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section3.4.3'>3.4.3. Preliminary Data Handling</a>\n",
    "Let's quality of the dataset by looking first and last rows in pandas using the [head()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) and [tail()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first five rows of the raw DataFrame, df_fbref_goalkeeper_raw\n",
    "df_fbref_goalkeeper_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last five rows of the raw DataFrame, df_fbref_goalkeeper_raw\n",
    "df_fbref_goalkeeper_raw.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[shape](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html) returns a tuple representing the dimensionality of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the raw DataFrame, df_fbref_goalkeeper_raw\n",
    "print(df_fbref_goalkeeper_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[columns](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html) returns the column labels of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (column names) of the raw DataFrame, df_fbref_goalkeeper_raw\n",
    "df_fbref_goalkeeper_raw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [dtypes](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html) method returns the data types of each attribute in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays all columns\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(df_fbref_goalkeeper_raw.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [info](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html) method to get a quick description of the data, in particular the total number of rows, and each attributeâ€™s type and number of non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info for the raw DataFrame, df_fbref_goalkeeper_raw\n",
    "df_fbref_goalkeeper_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [describe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) method to show some useful statistics for each numerical column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the raw DataFrame, df_fbref_goalkeeper_raw, showing some summary statistics for each numberical column in the DataFrame\n",
    "df_fbref_goalkeeper_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will check to see how many missing values we have i.e. the number of NULL values in the dataset, and in what features these missing values are located. This can be plotted nicely using the [missingno](https://pypi.org/project/missingno/) library (pip install missingno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot visualisation of the missing values for each feature of the raw DataFrame, df_fbref_goalkeeper_raw\n",
    "msno.matrix(df_fbref_goalkeeper_raw, figsize = (30, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts of missing values\n",
    "null_value_stats = df_fbref_goalkeeper_raw.isnull().sum(axis=0)\n",
    "null_value_stats[null_value_stats != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualisation shows us very quickly that there are missing values in the dataset but as this data is scraped, this fine at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a id='section4'></a>\n",
    "\n",
    "## <a id='#section4'>4. Summary</a>\n",
    "This notebook scrapes player statstics data from [FBref](https://fbref.com/en/) via [StatsBomb](https://statsbomb.com/), using [pandas](http://pandas.pydata.org/) for data manipulation through DataFrames.\n",
    "\n",
    "With this notebook we now have aggregated player performance data for players in the 'Big 5' European leagues for the 17/18-present seasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a id='section5'></a>\n",
    "\n",
    "## <a id='#section5'>5. Next Steps</a>\n",
    "This data is now ready to be engineered before being matched to other datasets such as data from [TransferMarkt](https://www.transfermarkt.co.uk/) and [Capology](https://www.capology.com/).\n",
    "\n",
    "The Data Engineering subfolder in GitHub can be found [here](https://github.com/eddwebster/football_analytics/tree/master/notebooks/B\\)%20Data%20Engineering) and a static version of the FBref data engineering notebookecord can be found [here](https://nbviewer.org/github/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/FBref%20Player%20Stats%20Data%20Engineering.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a id='section6'></a>\n",
    "\n",
    "## <a id='#section6'>6. References</a>\n",
    "\n",
    "#### Data and Web Scraping\n",
    "*    [FBref](https://fbref.com/) for the data to scrape\n",
    "*    FBref statement for using StatsBomb's data: https://fbref.com/en/statsbomb/\n",
    "*    [StatsBomb](https://statsbomb.com/) providing the data to FBref\n",
    "*    [FBref_EPL GitHub repository](https://github.com/chmartin/FBref_EPL) by [chmartin](https://github.com/chmartin) for the original web scraping code\n",
    "*    [Scrape-FBref-data GitHub repository](https://github.com/parth1902/Scrape-FBref-data) by [parth1902](https://github.com/parth1902) for the revised web scraping code for the new FBref metrics\n",
    "\n",
    "\n",
    "#### Countries\n",
    "*    [Comparison of alphabetic country codes Wiki](https://en.wikipedia.org/wiki/Comparison_of_alphabetic_country_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "***Visit my website [eddwebster.com](https://www.eddwebster.com) or my [GitHub Repository](https://github.com/eddwebster) for more projects. If you'd like to get in contact, my Twitter handle is [@eddwebster](http://www.twitter.com/eddwebster) and my email is: edd.j.webster@gmail.com.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the top](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 642,
   "position": {
    "height": "664px",
    "left": "1119px",
    "right": "20px",
    "top": "-7px",
    "width": "489px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
